name: Presubmit Benchmarks
permissions:
  contents: read
on:
  workflow_dispatch:
    inputs:
      halt-for-connection:
        description: 'Should this workflow run wait for a remote connection?'
        type: choice
        required: true
        default: 'no'
        options:
        - 'yes'
        - 'no'
  pull_request:
  push:
    branches:
      - dev

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}
  cancel-in-progress: ${{ github.ref != 'main' }}

jobs:
  Tests:
    strategy:
      # Don't fail fast - want to see results for all builds even if one fails.
      fail-fast: false
      matrix:
        job_info: [
          {
            pool: "linux-x86-n2-16",
            container: "us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest",
            pretty_name: "XLA Linux x86 CPU 16 vcpu Presubmit",
            bazel_arch_dir: "k8-opt",
            platform: "CPU"
          },
          {
            pool: "arc-linux-arm64-c4a-64-dev", #"linux-arm64-c4a-16",
            container: "us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest",
            pretty_name: "XLA Linux ARM64 CPU DEV",
            bazel_arch_dir: "aarch64-opt",
            platform: "CPU"
          },
          {
            pool: "linux-x86-n2-128",
            container: "us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest",
            pretty_name: "XLA Linux x86 CPU 128 vcpu Presubmit",
            bazel_arch_dir: "k8-opt",
            platform: "CPU"
          },
          {
            pool: "linux-x86-g2-16-l4-1gpu",
            container: "us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest",
            pretty_name: "XLA Linux x86 GPU T4 16 vcpu Presubmit",
            bazel_arch_dir: "k8-opt",
            platform: "GPU"
          },
        ]
    name: ${{ matrix.job_info.pretty_name }}
    runs-on: ${{ matrix.job_info.pool }}
    container: ${{ matrix.job_info.container }}
    defaults:
      run:
        shell: bash
    timeout-minutes: 10
    steps:
      - name: Checkout OpenXLA
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        
      - name: Print Git Information
        run: |
          echo "Commit SHA: ${{ github.sha }}"
          echo "Head SHA (PR): ${{ github.event.pull_request.head.sha }}"
          echo "Head Branch (PR): ${{ github.head_ref }}"
          echo "Pull Request Number: ${{ github.event.number }}"

      - name: "Run build.py"
        run: |
          ./build_tools/ci/build.py --build="${{ matrix.job_info.pretty_name }}_github_actions"
            

      # Run the corresponding HLO tests based on platform
      - name: Run HLO tests
        run: |
          bazel_arch_dir="${{ matrix.job_info.bazel_arch_dir }}"  # Get directory from matrix
          binary_path="./bazel-out/${bazel_arch_dir}/bin/xla/tools/run_hlo_module"
          echo "Running test with binary: $binary_path"
          if [[ -x "$binary_path" ]]; then
            echo "Running test with binary: $binary_path"
            if [[ "${{ matrix.job_info.platform }}" == "CPU" ]]; then
              output=$($binary_path --input_format=hlo --reference_platform="" --platform="${{ matrix.job_info.platform }}" xla/tools/hlo_opt/tests/cpu_llvm.hlo)
              echo "$output" > output.txt
              gsutil cp output.txt gs://juliagmt/cpu_llvm_hlo_test_output_${{ matrix.job_info.pool }}.txt
            elif [[ "${{ matrix.job_info.platform }}" == "GPU" ]]; then
              output=$($binary_path --input_format=hlo --reference_platform="" --platform="${{ matrix.job_info.platform }}" xla/tools/hlo_opt/tests/gpu_hlo_llvm.hlo)
              echo "$output" > output.txt
              gsutil cp output.txt gs://juliagmt/gpu_llvm_hlo_test_output_${{ matrix.job_info.pool }}.txt
            else
              echo "Unsupported platform: ${{ matrix.job_info.platform }}"
              exit 1
            fi
          else
            echo "Error: Binary not found at expected location: $binary_path"
            exit 1
          fi
    
